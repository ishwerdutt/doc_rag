-----

## üê≥ Running with Docker

You can run this project in a containerized environment using Docker Compose. This setup leverages **Python 3.13 (slim)** and installs all dependencies within a virtual environment inside the container.

-----

## üöÄ Our RAG Pipeline

This project implements a Retrieval-Augmented Generation (RAG) pipeline designed to answer questions based on a corpus of clinical equipment documents. Here's how it works:

### 1\. Document Ingestion and Chunking

  * **PDF Loading:** The pipeline starts by loading PDF documents from the specified `data/pdfs` directory using `PyPDFLoader`.
  * **Text Splitting:** Loaded documents are then split into smaller, manageable chunks using `RecursiveCharacterTextSplitter`. This ensures that individual pieces of text are small enough for embedding and retrieval, with a configured `chunk_size` and `chunk_overlap` for context preservation.

### 2\. Embedding and Vector Storage

  * **HuggingFace Embeddings:** Each text chunk is converted into a numerical vector (embedding) using the `BAAI/bge-large-en-v1.5` HuggingFace embeddings model. This model is chosen for its strong performance in semantic search.
  * **FAISS Vector Store:** These embeddings are then stored in a **FAISS (Facebook AI Similarity Search)** vector index. FAISS is an efficient library for similarity search and clustering of dense vectors. The index is persistent and can be loaded from or saved to `data/faiss_index`, avoiding re-processing documents on subsequent runs.

### 3\. Language Model (LLM) Integration

  * **Gemini API:** The core language model is powered by Google's **Gemini API**, specifically using the `gemma-3n-e4b-it` model. This model is responsible for generating coherent and relevant answers based on the retrieved context.
  * **API Key Management:** Your `GOOGLE_API_KEY` is essential for authenticating with the Gemini API.

### 4\. Retrieval Mechanism

  * **MultiQueryRetriever:** When a user poses a question, a `MultiQueryRetriever` is employed. This intelligent retriever generates multiple different versions of the original user query, which helps to retrieve a broader and more diverse set of relevant documents from the FAISS vector store.
  * **Top-K Retrieval:** For each generated sub-query, the system retrieves the top 5 most semantically similar document chunks from the FAISS index.

### 5\. Answer Generation (RAG Chain)

  * **Contextual Prompting:** The retrieved document chunks, along with the original user query, are then passed to the Gemini LLM as part of a carefully constructed prompt. This prompt instructs the LLM to use the provided context to formulate an answer.
  * **`create_retrieval_chain`:** LangChain's `create_retrieval_chain` orchestrates this process, automatically handling the retrieval of documents and feeding them into the LLM for answer generation.
  * **Identity Interception:** For common identity-related questions (e.g., "who are you?", "what are you?"), the system provides a predefined answer ("I am a bot designed to answer questions based on clinical equipment documents.") directly, bypassing the RAG pipeline for efficiency and accuracy on self-referential queries.

-----

## ‚öôÔ∏è Setup and Configuration

### 1\. Build and Start the App

To get started, simply run the following command in your project root:

```bash
docker compose up --build
```

This command will:

  * Build the Docker image for your application.
  * Start the Flask app using Gunicorn, which will be accessible on **port 5000**.

### 2\. Environment Variables

Ensure these environment variables are set for the app to function correctly:

  * `GOOGLE_API_KEY`: **Required** for accessing the Gemini API.
  * `PDF_DATA_PATH`: Specifies the directory where your PDF documents are stored (default: `data/pdfs`).
  * `FAISS_INDEX_PATH`: Defines the location for saving/loading the FAISS index files (default: `data/faiss_index`).

**Recommendation:** Create a `.env` file in your project's root directory (e.g., `.env`) and uncomment the `env_file` line in `docker-compose.yml` to automatically load these variables.

### 3\. Ports

  * The application is exposed on **port 5000** within the container and is mapped to `localhost:5000` on your host machine by default.

### 4\. Data and Persistence

  * Place your clinical equipment PDF files into the `data/pdfs/` directory.
  * The FAISS index files will be stored in `data/faiss_index/`.
  * For development and data persistence, it's recommended to **mount these directories as Docker volumes**. You can enable the `volumes` section in your `docker-compose.yml` file to achieve this.

### 5\. Customization

  * **Multi-stage Dockerfile:** The provided `Dockerfile` uses a multi-stage build, which helps create smaller, more efficient images and ensures non-root execution for enhanced security.
  * **Port Changes:** If you need to change the application's port or other runtime settings, update the `EXPOSE` and `CMD` lines within the `Dockerfile` and adjust the `ports` section in `docker-compose.yml` accordingly.

-----
